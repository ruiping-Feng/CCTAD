{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "dir='/home/houshiyuan/frp/CCTAD'\n",
    "Dataset='HIC056'   \n",
    "resolution=50    \n",
    "output_dir=dir+'/output/'+Dataset+f'/{resolution}kb'\n",
    "models_dir=dir+'/config/models/'+Dataset+f'/{resolution}kb'\n",
    "best_seed_dir=dir+'/config/best_seed'\n",
    "final_output=dir+'/final_output/'+Dataset+f'/{resolution}kb'\n",
    "path='/mnt/sdi/frp/data/marks'  \n",
    "hic_matrix_dir='/mnt/sdi/frp/data/hic_matrix'  \n",
    "method='ConvCluster'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c79627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(final_output, exist_ok=True)\n",
    "os.makedirs(best_seed_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(final_output, \"TAD\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(final_output, \"TAD_res\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c941dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ConvAutoencoder1D(nn.Module):\n",
    "    def __init__(self, input_dim=100, embedding_dim=16):\n",
    "        super(ConvAutoencoder1D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim * 4, embedding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, input_dim * 4),\n",
    "            nn.Unflatten(1, (4, input_dim)),\n",
    "            nn.ConvTranspose1d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=5, padding=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "def create_connectivity(n):\n",
    "    mat = lil_matrix((n, n))\n",
    "    for i in range(n - 1):\n",
    "        mat[i, i + 1] = mat[i + 1, i] = 1\n",
    "    return mat\n",
    "\n",
    "def run_clustering(features, distance_threshold=1.5):\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=distance_threshold,\n",
    "        linkage='average',\n",
    "        connectivity=create_connectivity(len(features))\n",
    "    )\n",
    "    return clustering.fit_predict(features)\n",
    "\n",
    "def train_model(hic_matrix, chrom,input_dim=100, embedding_dim=16, num_epochs=500, lr=1e-4,load_model=False):   \n",
    "    \n",
    "    features = StandardScaler().fit_transform(hic_matrix)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    model = ConvAutoencoder1D(input_dim=input_dim, embedding_dim=embedding_dim).to(device)\n",
    "    \n",
    "    if load_model:\n",
    "        best_model_path = models_dir+f'/best_model_{chrom}.pt'\n",
    "        if os.path.exists(best_model_path):\n",
    "            print(\"加载已有模型参数：\", best_model_path)\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        else:\n",
    "            print(\"未找到模型文件，无法加载。\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                _, z = model(features_tensor)\n",
    "        return model, z\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            recon, z = model(features_tensor)\n",
    "            recon_loss = F.mse_loss(recon.squeeze(1), features_tensor.squeeze(1))\n",
    "            recon_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        return model, z\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa314c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tad_boundaries(cluster_labels):  \n",
    "    boundaries = []\n",
    "    for i in range(1, len(cluster_labels)):\n",
    "        if cluster_labels[i] != cluster_labels[i - 1]:\n",
    "            boundaries.append(i)\n",
    "    return boundaries\n",
    "\n",
    "def create_boundary_labels_from_clusters(cluster_labels, hic_len, save_path):  \n",
    "    labels = np.zeros(hic_len, dtype=int)\n",
    "    boundary_bins = []\n",
    "\n",
    "    for i in range(1, len(cluster_labels)):\n",
    "        if cluster_labels[i] != cluster_labels[i - 1]:\n",
    "            labels[i] = 1\n",
    "            boundary_bins.append(i)\n",
    "\n",
    "    np.savetxt(save_path, labels, fmt='%d')\n",
    "    \n",
    "    return boundary_bins\n",
    "\n",
    "def merge_consecutive_small_tads(input_file, output_file, min_bin_size=3):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = [list(map(int, line.strip().split())) for line in f if line.strip()]\n",
    "\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        start, end = lines[i]\n",
    "        if (end - start) < min_bin_size:\n",
    "           \n",
    "            merge_start = start\n",
    "            merge_end = end\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i][0] == merge_end and (lines[i][1] - lines[i][0]) < min_bin_size:\n",
    "                merge_end = lines[i][1]\n",
    "                i += 1\n",
    "            merged.append((merge_start, merge_end))\n",
    "        else:\n",
    "            \n",
    "            merged.append((start, end))\n",
    "            i += 1\n",
    "    with open(output_file, 'w') as f:\n",
    "        for s, e in merged:\n",
    "            f.write(f\"{s} {e}\\n\")\n",
    "\n",
    "def process_tad_file(input_path, output_path, min_size=2):\n",
    "    def read_intervals_from_file(file_path):\n",
    "        intervals = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    intervals.append([int(parts[0]), int(parts[1])])\n",
    "        return intervals\n",
    "    def write_intervals_to_file(intervals, output_path):\n",
    "        with open(output_path, 'w') as f:\n",
    "            for interval in intervals:\n",
    "                f.write(f\"{interval[0]} {interval[1]}\\n\")\n",
    "\n",
    "    def merge_tads(tad_intervals, min_size):\n",
    "        if not tad_intervals:\n",
    "            return []\n",
    "        merged = []\n",
    "        i = 0\n",
    "        while i < len(tad_intervals):\n",
    "            curr = tad_intervals[i]\n",
    "            start, end = curr\n",
    "            length = end - start\n",
    "\n",
    "            if length < min_size:\n",
    "                \n",
    "                if merged and merged[-1][1] == start:\n",
    "                    merged[-1][1] = end\n",
    "               \n",
    "                elif i + 1 < len(tad_intervals) and tad_intervals[i + 1][0] == end:\n",
    "                    tad_intervals[i + 1][0] = start      \n",
    "            else:\n",
    "                merged.append([start, end])\n",
    "            i += 1\n",
    "\n",
    "        return merged\n",
    " \n",
    "    intervals = read_intervals_from_file(input_path)\n",
    "    merged_intervals = merge_tads(intervals, min_size)\n",
    "    write_intervals_to_file(merged_intervals, output_path)\n",
    "    \n",
    "def convert_boundary_file_to_tads(input_file, output_file, start=1):\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        boundaries = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "    current = start\n",
    "    with open(output_file, 'w') as f:\n",
    "        for b in boundaries:\n",
    "            f.write(f\"{current} {b}\\n\")\n",
    "            current = b\n",
    "            \n",
    "def convert_tad_resolution(input_file, output_file, resolution=25000):\n",
    "    tads = np.loadtxt(input_file, dtype=int)\n",
    "    if tads.ndim == 1:\n",
    "        tads = np.expand_dims(tads, axis=0)\n",
    "\n",
    "    tads_bp = tads * resolution\n",
    "    np.savetxt(output_file, tads_bp, fmt=\"%d\", delimiter=\"\\t\")\n",
    " \n",
    "def mark_label_transitions(labels):\n",
    "    labels = np.array(labels)\n",
    "    transitions = np.zeros_like(labels)\n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != labels[i - 1]:\n",
    "            transitions[i] = 1\n",
    "            transitions[i - 1] = 1  \n",
    "    return transitions\n",
    "def mark_transitions_from_file(input_file, output_file):\n",
    " \n",
    "    with open(input_file, 'r') as f:\n",
    "        labels = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "    transitions = mark_label_transitions(labels)\n",
    "    np.savetxt(output_file, transitions, fmt='%d')\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e251fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "def tad(hic_matrix,chrom,seed,load_model,threshold):\n",
    "\n",
    "    if seed is not None:\n",
    "        seed_everything(seed) \n",
    "    trained_model, z = train_model(hic_matrix, chrom,input_dim=hic_matrix.shape[1],num_epochs=500,load_model=load_model)    \n",
    "    z_np = z.detach().cpu().numpy()\n",
    "    cluster_labels = run_clustering(z_np, distance_threshold=threshold)  \n",
    "   \n",
    "    if all(x == 0 for x in cluster_labels):\n",
    "        return 0,0,0,None\n",
    "    cluster_labels_path=os.path.join(output_dir, \"cluster_labels\", f\"{chrom}_cluster_labels.txt\")  \n",
    "    os.makedirs(os.path.dirname(cluster_labels_path), exist_ok=True)\n",
    "    np.savetxt(cluster_labels_path, cluster_labels, fmt=\"%d\")\n",
    "\n",
    "    boundary=identify_tad_boundaries(cluster_labels)\n",
    "    boundary_file=os.path.join(output_dir, \"boundary\", f\"boundary_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(boundary_file), exist_ok=True)\n",
    "    np.savetxt(boundary_file, boundary, fmt=\"%d\")\n",
    "\n",
    "    tad_path=os.path.join(output_dir, \"TAD\", f\"{Dataset}_{method}_{resolution}k_KR.{chrom}\")\n",
    "    os.makedirs(os.path.dirname(tad_path), exist_ok=True)\n",
    "    convert_boundary_file_to_tads(boundary_file, tad_path, start=1)\n",
    "\n",
    "    merge_consecutive_small_tads(tad_path,tad_path,min_bin_size=3)  \n",
    "    process_tad_file(tad_path,tad_path,min_size=3)\n",
    "    \n",
    "\n",
    "    res_tad_path=os.path.join(output_dir, \"TAD_res\", f\"{Dataset}_{method}_{resolution}k_KR.{chrom}\")\n",
    "    os.makedirs(os.path.dirname(res_tad_path), exist_ok=True)\n",
    "    convert_tad_resolution(tad_path, res_tad_path, resolution=resolution*1000)\n",
    "\n",
    "    os.makedirs(os.path.join(output_dir, \"labels\"), exist_ok=True)\n",
    "    mark_transitions_from_file(cluster_labels_path, output_dir+f\"/labels/01_{chrom}_cluster_labels.txt\")\n",
    "\n",
    "    return trained_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc24263",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromosomes = [f\"chr{i}\" for i in range(21,22)]   #染色体编号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理HIC056_chr21,50kb\n",
      "模型已经保存到： /home/houshiyuan/frp/CCTAD/config/models/HIC056/50kb/best_model_chr21.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import shutil\n",
    "for chrom in chromosomes:\n",
    "    print(f\"正在处理{Dataset}_{chrom},{resolution}kb\")\n",
    "    # 读取数据\n",
    "    hic_matrix = np.loadtxt( f\"{hic_matrix_dir}/{Dataset}/{resolution}kb/{Dataset}_{resolution}k_KR.{chrom}\") \n",
    "\n",
    "    k=2    # k==1 训练 \n",
    "            # k==2 根据随机种子复现\n",
    "            # k==其他 直接调用训练好的最优模型\n",
    "    if k==1:\n",
    "        ### 训练\n",
    "        load_model=False  \n",
    "        # 指定最优阈值\n",
    "        best_threshold=1.2   \n",
    "        seed = int(time.time())  \n",
    "        trained_model=tad(hic_matrix,chrom,seed,load_model,best_threshold)\n",
    "        #print(seed)\n",
    "        torch.save(trained_model.state_dict(), models_dir+f\"/model_{chrom}.pt\")\n",
    "        shutil.copy(output_dir+f'/TAD/{Dataset}_{method}_{resolution}k_KR.{chrom}', final_output+f'/TAD/{Dataset}_{method}_{resolution}k_KR.{chrom}')\n",
    "        shutil.copy(output_dir+f'/TAD_res/{Dataset}_{method}_{resolution}k_KR.{chrom}', final_output+f'/TAD_res/{Dataset}_{method}_{resolution}k_KR.{chrom}')\n",
    "        # with open(os.path.join(best_seed_dir, f\"{Dataset}_{resolution}kb_best_seed.txt\"), \"a\") as f:\n",
    "        #     f.write(f\"{chrom}:{seed}\\n\")\n",
    "    elif k==2:\n",
    "        ### 根据最优随机种子再次训练最优模型 训练一次\n",
    "        load_model=False   \n",
    "        seed=1755498234    # 某条染色体对应最优模型的随机种子\n",
    "        best_threshold=1.2  #最优阈值\n",
    "        trained_model=tad(hic_matrix,chrom,seed,load_model,best_threshold)\n",
    "\n",
    "        p=0     \n",
    "        if p==1:  #是否保存本次训练的模型  p==1 保存，否则不保存\n",
    "            torch.save(trained_model.state_dict(), models_dir+f\"/best_model_{chrom}.pt\")\n",
    "            print(\"模型已经保存到：\",models_dir+f\"/best_model_{chrom}.pt\")\n",
    "            shutil.copy(output_dir+f'/TAD/{Dataset}_{method}_{resolution}k_KR.{chrom}', final_output+f'/TAD/{Dataset}_{method}_{resolution}k_KR.{chrom}')\n",
    "            shutil.copy(output_dir+f'/TAD_res/{Dataset}_{method}_{resolution}k_KR.{chrom}', final_output+f'/TAD_res/{Dataset}_{method}_{resolution}k_KR.{chrom}')\n",
    "\n",
    "    else:\n",
    "        ### 直接调用训练好的最优模型\n",
    "        load_model=True  \n",
    "        seed=None\n",
    "        best_threshold=1.2    #某条染色体对应最优阈值\n",
    "        tad(hic_matrix,chrom,seed,load_model,best_threshold)  \n",
    "        shutil.copy(output_dir+f'/TAD/{Dataset}_{method}_{resolution}k_KR.{chrom}', final_output+f'/TAD/{Dataset}_{method}_{resolution}k_KR.{chrom}')\n",
    "        shutil.copy(output_dir+f'/TAD_res/{Dataset}_{method}_{resolution}k_KR.{chrom}', final_output+f'/TAD_res/{Dataset}_{method}_{resolution}k_KR.{chrom}')\n",
    "\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d467c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
