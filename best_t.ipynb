{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "dir='/home/houshiyuan/frp/CCTAD'\n",
    "Dataset='HIC002'   \n",
    "resolution=50\n",
    "output_dir=dir+'/output/'+Dataset+f'/{resolution}kb'\n",
    "models_dir=dir+'/config/models/'+Dataset+f'/{resolution}kb'\n",
    "best_threshold_dir=dir+'/config/best_threshold'\n",
    "final_output=dir+'/final_output/'+Dataset+f'/{resolution}kb'\n",
    "path='/mnt/sdi/frp/data/marks'  \n",
    "hic_matrix_dir='/mnt/sdi/frp/data/hic_matrix'  \n",
    "\n",
    "chromosomes = [f\"chr{i}\" for i in range(1,23)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c79627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(final_output, exist_ok=True)\n",
    "os.makedirs(os.path.join(final_output, \"TAD\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(final_output, \"TAD_res\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c941dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ConvAutoencoder1D(nn.Module):\n",
    "    def __init__(self, input_dim=100, embedding_dim=16):\n",
    "        super(ConvAutoencoder1D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(8, 4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim * 4, embedding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, input_dim * 4),\n",
    "            nn.Unflatten(1, (4, input_dim)),\n",
    "            nn.ConvTranspose1d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(8, 1, kernel_size=5, padding=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "def create_connectivity(n):\n",
    "    mat = lil_matrix((n, n))\n",
    "    for i in range(n - 1):\n",
    "        mat[i, i + 1] = mat[i + 1, i] = 1\n",
    "    return mat\n",
    "\n",
    "def run_clustering(features, distance_threshold=1.5):\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=distance_threshold,\n",
    "        linkage='average',\n",
    "        connectivity=create_connectivity(len(features))\n",
    "    )\n",
    "    return clustering.fit_predict(features)\n",
    "\n",
    "def train_model(hic_matrix, chrom,input_dim=100, embedding_dim=16, num_epochs=500, lr=1e-4,load_model=False):   \n",
    "    features = StandardScaler().fit_transform(hic_matrix)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    model = ConvAutoencoder1D(input_dim=input_dim, embedding_dim=embedding_dim).to(device)\n",
    "    \n",
    "    if load_model:\n",
    "        best_model_path = models_dir+f'/best_model_{chrom}.pt'\n",
    "        if os.path.exists(best_model_path):\n",
    "            print(\"加载已有模型参数：\", best_model_path)\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        else:\n",
    "            print(\"未找到模型文件，无法加载。\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                _, z = model(features_tensor)\n",
    "        return model, z\n",
    "    \n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            optimizer.zero_grad()\n",
    "            recon, z = model(features_tensor)\n",
    "            recon_loss = F.mse_loss(recon.squeeze(1), features_tensor.squeeze(1))\n",
    "            recon_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        return model, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa314c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tad_boundaries(cluster_labels):  \n",
    "    boundaries = []\n",
    "    for i in range(1, len(cluster_labels)):\n",
    "        if cluster_labels[i] != cluster_labels[i - 1]:\n",
    "            boundaries.append(i)\n",
    "    return boundaries\n",
    "\n",
    "def create_boundary_labels_from_clusters(cluster_labels, hic_len, save_path):  \n",
    "    labels = np.zeros(hic_len, dtype=int)\n",
    "    boundary_bins = []\n",
    "\n",
    "    for i in range(1, len(cluster_labels)):\n",
    "        if cluster_labels[i] != cluster_labels[i - 1]:\n",
    "            labels[i] = 1\n",
    "            boundary_bins.append(i)\n",
    "\n",
    "    np.savetxt(save_path, labels, fmt='%d')\n",
    "   \n",
    "    return boundary_bins\n",
    "\n",
    "def merge_consecutive_small_tads(input_file, output_file, min_bin_size=3):\n",
    "    with open(input_file, 'r') as f:\n",
    "        lines = [list(map(int, line.strip().split())) for line in f if line.strip()]\n",
    "\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        start, end = lines[i]\n",
    "        if (end - start) < min_bin_size:           \n",
    "            merge_start = start\n",
    "            merge_end = end\n",
    "            i += 1\n",
    "            while i < len(lines) and lines[i][0] == merge_end and (lines[i][1] - lines[i][0]) < min_bin_size:\n",
    "                merge_end = lines[i][1]\n",
    "                i += 1\n",
    "            merged.append((merge_start, merge_end))\n",
    "        else: \n",
    "            merged.append((start, end))\n",
    "            i += 1\n",
    "    with open(output_file, 'w') as f:\n",
    "        for s, e in merged:\n",
    "            f.write(f\"{s} {e}\\n\")\n",
    "\n",
    "def process_tad_file(input_path, output_path, min_size=2):\n",
    "    def read_intervals_from_file(file_path):\n",
    "        intervals = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 2:\n",
    "                    intervals.append([int(parts[0]), int(parts[1])])\n",
    "        return intervals\n",
    "\n",
    "    def write_intervals_to_file(intervals, output_path):\n",
    "        with open(output_path, 'w') as f:\n",
    "            for interval in intervals:\n",
    "                f.write(f\"{interval[0]} {interval[1]}\\n\")\n",
    "\n",
    "    def merge_tads(tad_intervals, min_size):\n",
    "        if not tad_intervals:\n",
    "            return []\n",
    "        merged = []\n",
    "        i = 0\n",
    "        while i < len(tad_intervals):\n",
    "            curr = tad_intervals[i]\n",
    "            start, end = curr\n",
    "            length = end - start\n",
    "            if length < min_size:              \n",
    "                if merged and merged[-1][1] == start:\n",
    "                    merged[-1][1] = end               \n",
    "                elif i + 1 < len(tad_intervals) and tad_intervals[i + 1][0] == end:\n",
    "                    tad_intervals[i + 1][0] = start               \n",
    "            else:\n",
    "                merged.append([start, end])\n",
    "            i += 1\n",
    "        return merged\n",
    " \n",
    "    intervals = read_intervals_from_file(input_path)\n",
    "    merged_intervals = merge_tads(intervals, min_size)\n",
    "    write_intervals_to_file(merged_intervals, output_path)    \n",
    "\n",
    "def convert_boundary_file_to_tads(input_file, output_file, start=1):\n",
    "    with open(input_file, 'r') as f:\n",
    "        boundaries = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "    current = start\n",
    "    with open(output_file, 'w') as f:\n",
    "        for b in boundaries:\n",
    "            f.write(f\"{current} {b}\\n\")\n",
    "            current = b\n",
    "            \n",
    "def convert_tad_resolution(input_file, output_file, resolution=25000):\n",
    "    tads = np.loadtxt(input_file, dtype=int)\n",
    "    if tads.ndim == 1:\n",
    "        tads = np.expand_dims(tads, axis=0)\n",
    "\n",
    "    tads_bp = tads * resolution\n",
    "    np.savetxt(output_file, tads_bp, fmt=\"%d\", delimiter=\"\\t\")\n",
    "  \n",
    "def mark_label_transitions(labels):\n",
    "    labels = np.array(labels)\n",
    "    transitions = np.zeros_like(labels)\n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != labels[i - 1]:\n",
    "            transitions[i] = 1\n",
    "            transitions[i - 1] = 1  \n",
    "    return transitions\n",
    "def mark_transitions_from_file(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        labels = [int(line.strip()) for line in f if line.strip().isdigit()]\n",
    "\n",
    "    transitions = mark_label_transitions(labels)\n",
    "    np.savetxt(output_file, transitions, fmt='%d')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1494c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_tad_intervals(filepath):\n",
    "    tad_bins = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            start, end = map(int, line.strip().split())\n",
    "     \n",
    "            bins = list(range(start, end))\n",
    "            tad_bins.append(bins)\n",
    "    return tad_bins\n",
    "\n",
    "def compute_intra_contact(tad_bins, hic_matrix):\n",
    "    if len(tad_bins) <= 1:\n",
    "        return 0.0\n",
    "    sub_matrix = hic_matrix[np.ix_(tad_bins, tad_bins)]\n",
    "    tril_indices = np.tril_indices_from(sub_matrix, k=-1)\n",
    "    values = sub_matrix[tril_indices]\n",
    "    return np.nanmean(values) if len(values) > 0 else 0.0\n",
    "\n",
    "def compute_inter_contact(tad_bins, neighbor_bins, hic_matrix, boundary_size=2):\n",
    "    if len(tad_bins) == 0 or len(neighbor_bins) == 0:\n",
    "        return 0.0\n",
    "    left_edge = tad_bins[:boundary_size]\n",
    "    right_edge = neighbor_bins[-boundary_size:]\n",
    "    sub_matrix = hic_matrix[np.ix_(left_edge, right_edge)]\n",
    "    return np.nanmean(sub_matrix) if sub_matrix.size > 0 else 0.0\n",
    "\n",
    "def evaluate_tad_quality(tads, hic_matrix, boundary_size=2):\n",
    "    quality_scores = []\n",
    "    for i, tad in enumerate(tads):\n",
    "        intra = compute_intra_contact(tad, hic_matrix)\n",
    "        left = tads[i - 1] if i > 0 else []\n",
    "        right = tads[i + 1] if i < len(tads) - 1 else []\n",
    "        inter_left = compute_inter_contact(tad, left, hic_matrix, boundary_size)\n",
    "        inter_right = compute_inter_contact(tad, right, hic_matrix, boundary_size)\n",
    "\n",
    "        inters = [v for v in [inter_left, inter_right] if v > 0]\n",
    "        inter = np.mean(inters) if inters else 0\n",
    "        quality = intra - inter\n",
    "        quality_scores.append(quality)\n",
    "\n",
    "    average_score = np.mean(quality_scores) if quality_scores else 0\n",
    "    return quality_scores, average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e251fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "def tad(hic_matrix,chrom,seed,load_model,threshold):\n",
    "    if seed is not None:\n",
    "        seed_everything(seed) \n",
    "    trained_model, z = train_model(hic_matrix, chrom,input_dim=hic_matrix.shape[1],num_epochs=500,load_model=load_model)    \n",
    "    z_np = z.detach().cpu().numpy()\n",
    "    cluster_labels = run_clustering(z_np, distance_threshold=threshold)   \n",
    "   \n",
    "    if all(x == 0 for x in cluster_labels):\n",
    "        return 0,None\n",
    "    cluster_labels_path=os.path.join(output_dir, \"cluster_labels\", f\"{chrom}_cluster_labels.txt\") \n",
    "    os.makedirs(os.path.dirname(cluster_labels_path), exist_ok=True)\n",
    "    np.savetxt(cluster_labels_path, cluster_labels, fmt=\"%d\")\n",
    "  \n",
    "    boundary=identify_tad_boundaries(cluster_labels)\n",
    "    boundary_file=os.path.join(output_dir, \"boundary\", f\"boundary_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(boundary_file), exist_ok=True)\n",
    "    np.savetxt(boundary_file, boundary, fmt=\"%d\")\n",
    "\n",
    "    tad_path=os.path.join(output_dir, \"TAD\", f\"tads_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(tad_path), exist_ok=True)\n",
    "    convert_boundary_file_to_tads(boundary_file, tad_path, start=1)\n",
    "\n",
    "    merge_consecutive_small_tads(tad_path,tad_path,min_bin_size=3)  \n",
    "    process_tad_file(tad_path,tad_path,min_size=3)\n",
    "    \n",
    "    res_tad_path=os.path.join(output_dir, \"TAD_res\", f\"res_tads_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(res_tad_path), exist_ok=True)\n",
    "    convert_tad_resolution(tad_path, res_tad_path, resolution=resolution*1000)\n",
    "    return None,trained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc24263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold( hic_matrix,chrom, thresholds):\n",
    "    \n",
    "    cluster_labels_path=os.path.join(output_dir, \"cluster_labels\", f\"{chrom}_cluster_labels.txt\")  \n",
    "    os.makedirs(os.path.dirname(cluster_labels_path), exist_ok=True)\n",
    "    boundary_file=os.path.join(output_dir, \"boundary\", f\"boundary_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(boundary_file), exist_ok=True)\n",
    "    tad_path=os.path.join(output_dir, \"TAD\", f\"tads_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(tad_path), exist_ok=True)\n",
    "    res_tad_path=os.path.join(output_dir, \"TAD_res\", f\"res_tads_{chrom}.txt\")\n",
    "    os.makedirs(os.path.dirname(res_tad_path), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"labels\"), exist_ok=True)\n",
    "\n",
    "    #模型训练\n",
    "    trained_model, z = train_model(hic_matrix, chrom,input_dim=hic_matrix.shape[1],num_epochs=500,load_model=False)    \n",
    "    z_np = z.detach().cpu().numpy()\n",
    "    best_score=0\n",
    "    for threshold in thresholds:\n",
    "        cluster_labels = run_clustering(z_np, distance_threshold=threshold)   \n",
    "        if all(x == 0 for x in cluster_labels):\n",
    "            continue \n",
    "        boundary=identify_tad_boundaries(cluster_labels)\n",
    "        np.savetxt(boundary_file, boundary, fmt=\"%d\")\n",
    "        convert_boundary_file_to_tads(boundary_file, tad_path, start=1)\n",
    "        merge_consecutive_small_tads(tad_path,tad_path,min_bin_size=3)  \n",
    "        process_tad_file(tad_path,tad_path,min_size=3)\n",
    "        tads = load_tad_intervals(tad_path)  \n",
    "        # 计算质量评分\n",
    "        scores, avg = evaluate_tad_quality(tads, hic_matrix, boundary_size=2)\n",
    "        if avg > best_score:\n",
    "            best_score = avg\n",
    "            best_t = threshold\n",
    "        #print(f\"{chrom}聚类阈值为：{threshold:.1f}，TAD质量评分：{avg:.4f}\")\n",
    "    #print(f\"{chrom}最优聚类阈值为：{best_t:.1f}，TAD质量评分：{best_score:.4f}\")\n",
    "    return best_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67146238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "all_thresholds = []\n",
    "for chrom in chromosomes:\n",
    "    hic_matrix = np.loadtxt( f\"{hic_matrix_dir}/{Dataset}/{resolution}kb/{Dataset}_{resolution}k_KR.{chrom}\")                    \n",
    "    best_threshold=find_best_threshold(hic_matrix,chrom, thresholds=np.arange(0.1, 2.0, 0.1))\n",
    "    all_thresholds.append(best_threshold)\n",
    "    print(f'{chrom}:{best_threshold:.2f}')\n",
    "    \n",
    "average_threshold = np.mean(all_thresholds)\n",
    "print(f'Average threshold across all chromosomes: {average_threshold:.3f}')\n",
    "\n",
    "with open(os.path.join(best_threshold_dir, f\"{Dataset}_{resolution}kb_best_threshold.txt\"), \"a\") as f:\n",
    "    f.write(f\"best_threshold:{average_threshold:.1f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
